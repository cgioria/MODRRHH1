# ðŸ“Š EVALUACIÃ“N DE MODELO

Herramientas para medir y evaluar el rendimiento del modelo de recruitment.

## ðŸ“ Contenido

```
evaluation/
â”œâ”€â”€ evaluate_model.py                 # Script principal de evaluaciÃ³n
â”œâ”€â”€ EVALUATION_REPORT.txt             # Reporte de evaluaciÃ³n
â”œâ”€â”€ evaluation_results.json           # Resultados en JSON
â”œâ”€â”€ HOW_TO_MEASURE_PERFORMANCE.py     # GuÃ­a de mediciÃ³n de rendimiento
â”œâ”€â”€ QUICK_PERFORMANCE_TEST.py         # Test rÃ¡pido de rendimiento
â””â”€â”€ README.md                         # Este archivo
```

## ðŸš€ Uso RÃ¡pido

### 1. Evaluar el Modelo Completo

```bash
cd evaluation
python evaluate_model.py
```

**Salida esperada:**
```
INFERENCE SPEED:
  â€¢ 100 textos en 4.49 segundos
  â€¢ 22 textos/segundo
  â€¢ 44.85ms por texto

SIMILARITY METRICS:
  â€¢ Similitud media: 0.7702 âœ…
  
SEARCH QUALITY:
  â€¢ MRR: 1.0000 âœ…âœ…âœ… EXCELENTE
  â€¢ NDCG: 0.9931 âœ…âœ…âœ… EXCELENTE
  
CLUSTERING:
  â€¢ CohesiÃ³n: 0.5427 âœ…

MULTILINGUAL:
  â€¢ English: 0.8649 âœ…
  â€¢ Spanish: 0.9482 âœ…âœ…
  â€¢ ... y mÃ¡s
```

### 2. Test RÃ¡pido (1 minuto)

```bash
python QUICK_PERFORMANCE_TEST.py
```

Prueba solo velocidad e inferencia bÃ¡sica.

### 3. Ver GuÃ­a Completa

```bash
# Python
python HOW_TO_MEASURE_PERFORMANCE.py

# Markdown
cat EVALUATION_REPORT.txt
```

## ðŸ“ˆ MÃ©tricas Disponibles

| MÃ©trica | Archivo | DescripciÃ³n |
|---------|---------|-------------|
| Velocidad de Inferencia | `evaluate_model.py` | Textos por segundo, latencia |
| Similitud | `evaluate_model.py` | Score de similitud promedio |
| BÃºsqueda (MRR, NDCG) | `evaluate_model.py` | Calidad de ranking |
| Clustering | `evaluate_model.py` | CohesiÃ³n de grupos |
| DistribuciÃ³n de Embeddings | `evaluate_model.py` | AnÃ¡lisis de embeddings |
| MultilingÃ¼e | `evaluate_model.py` | Rendimiento en 5+ idiomas |

## ðŸ” InterpretaciÃ³n de Resultados

### Velocidad
- âœ… Bueno: > 20 textos/seg en CPU
- âœ… Excelente: > 200 textos/seg en GPU

### Similitud
- âœ… Bueno: 0.7+ promedio
- âœ… Excelente: 0.8+ promedio

### BÃºsqueda (MRR)
- âœ… Bueno: > 0.8
- âœ… Excelente: > 0.95

### NDCG
- âœ… Bueno: > 0.9
- âœ… Excelente: > 0.98

## ðŸ“Š Archivo de Resultados

Los resultados se guardan automÃ¡ticamente en:
- `evaluation_results.json` - Datos estructurados
- `EVALUATION_REPORT.txt` - Reporte formateado

## ðŸŽ¯ PrÃ³ximos Pasos

1. **Ejecutar evaluaciÃ³n inicial**
   ```bash
   python evaluate_model.py
   ```

2. **Revisar resultados**
   ```bash
   cat EVALUATION_REPORT.txt
   ```

3. **Guardar baseline**
   ```bash
   cp evaluation_results.json evaluation_results_baseline.json
   ```

4. **Comparar despuÃ©s de cambios**
   ```bash
   # Ejecutar nuevo test
   python evaluate_model.py
   # Comparar con baseline
   diff evaluation_results.json evaluation_results_baseline.json
   ```

## ðŸ”§ Personalizar EvaluaciÃ³n

Edita `evaluate_model.py` para:
- Cambiar tamaÃ±o de muestra
- Agregar mÃ©tricascustomizadas
- Evaluar dominios especÃ­ficos
- Usar datos reales

## ðŸ“š InformaciÃ³n del Modelo

Evaluado: **paraphrase-multilingual-mpnet-base-v2**
- Dimensiones: 768
- Idiomas: 9
- EspecializaciÃ³n: Recruitment
- MRR Actual: 1.0000
- NDCG Actual: 0.9931

---

**Ãšltima evaluaciÃ³n:** Ver `EVALUATION_REPORT.txt`
