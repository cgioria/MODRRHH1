"""
QUICK START: EVALUAR MODELO EN 5 MINUTOS
"""

# =========================================================================
# OPCIÃ“N 1: EvaluaciÃ³n completa automÃ¡tica (RECOMENDADO)
# =========================================================================

"""
Simplemente ejecuta:

    python evaluate_model.py ./model

Esto:
âœ“ Mide velocidad de inferencia
âœ“ Prueba similitud entre textos
âœ“ EvalÃºa calidad de bÃºsqueda (ranking)
âœ“ Prueba clustering
âœ“ Analiza distribuciÃ³n de embeddings
âœ“ Prueba multilingÃ¼e
âœ“ Genera reporte en evaluation_results.json
"""


# =========================================================================
# OPCIÃ“N 2: Pruebas puntuales en Python
# =========================================================================

"""
Importar el evaluador:
"""

from evaluate_model import ModelEvaluator

# Crear evaluador
evaluator = ModelEvaluator("./model", device="cpu")


# Prueba 1: Velocidad
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("PRUEBA 1: Velocidad de procesamiento")
speed = evaluator.measure_inference_speed(num_texts=100)
print(f"  Velocidad: {speed['texts_per_second']:.0f} textos/segundo")
print(f"  Latencia: {speed['ms_per_text']:.2f} ms/texto\n")


# Prueba 2: Similitud
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("PRUEBA 2: Calidad de similitud")
similarity = evaluator.measure_similarity_metrics()
print(f"  PrecisiÃ³n: {similarity['accuracy_on_test_pairs']*100:.1f}%")
print(f"  Similitud media: {similarity['mean_similarity']:.4f}\n")


# Prueba 3: BÃºsqueda
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("PRUEBA 3: Calidad de ranking (bÃºsqueda)")
search = evaluator.measure_search_quality()
print(f"  MRR: {search['mrr']:.4f}")
print(f"  NDCG: {search['ndcg']:.4f}")
print(f"  Precision@5: {search['precisions']['precision@5']:.4f}\n")


# Prueba 4: Clustering
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("PRUEBA 4: Calidad de clustering")
clustering = evaluator.measure_clustering_quality()
print(f"  CohesiÃ³n: {clustering['average_cohesion']:.4f}\n")


# Prueba 5: Todos juntos
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("PRUEBA 5: EvaluaciÃ³n completa")
all_results = evaluator.run_full_evaluation()


# =========================================================================
# OPCIÃ“N 3: Pruebas especÃ­ficas del dominio (Recruitment)
# =========================================================================

"""
Para evaluar especÃ­ficamente para bÃºsqueda de candidatos:
"""

from loader import load_model

model = load_model("./model")


# Test 1: Matching simple
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
job_position = "Senior Python Developer with Machine Learning"
resume_text = "5 years python development, ml specialist"

score = model.similarity(job_position, resume_text)
print(f"Match score: {score:.2%}\n")


# Test 2: BÃºsqueda entre candidatos
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
query = "python backend engineer"

candidates = [
    "python developer 5 years backend",
    "java developer 10 years",
    "python engineer full stack",
    "frontend react developer",
    "devops kubernetes engineer",
]

results = model.search(query, candidates, top_k=3)

print("Top candidates:")
for i, result in enumerate(results, 1):
    print(f"  {i}. {result['candidate']}")
    print(f"     Score: {result['similarity']:.2%}\n")


# Test 3: AgrupaciÃ³n de perfiles
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
profiles = [
    "python developer",
    "python engineer",
    "senior python programmer",
    "frontend developer react",
    "frontend developer vue",
    "data scientist ml",
    "data engineer spark",
    "devops engineer",
]

clusters = model.cluster(profiles, n_clusters=4)

print("Clusters de perfiles:")
for cluster_id, group in clusters.items():
    print(f"\nGrupo {cluster_id}:")
    for profile in group:
        print(f"  - {profile}")


# =========================================================================
# OPCIÃ“N 4: Evaluar versus baseline
# =========================================================================

"""
Comparar con otro modelo (baseline):
"""

from sentence_transformers import SentenceTransformer

# Tu modelo entrenado
trained_model = load_model("./model")

# Modelo base sin entrenar
base_model = SentenceTransformer("sentence-transformers/paraphrase-multilingual-mpnet-base-v2")

# Test pair
text1 = "python developer"
text2 = "python engineer"

score_trained = trained_model.similarity(text1, text2)
score_base = base_model.similarity(text1, text2)

print(f"Similitud '{text1}' vs '{text2}':")
print(f"  Modelo base:     {score_base:.4f}")
print(f"  Modelo entrenado: {score_trained:.4f}")
print(f"  Mejora:          +{(score_trained - score_base):.4f} ({(score_trained/score_base - 1)*100:.1f}%)")


# =========================================================================
# OPCIÃ“N 5: Tests con datos reales
# =========================================================================

"""
Si tienes datos reales de tu dominio:
"""

# Ejemplo: CVs y descripciones de puestos
real_candidates = [
    "Ingeniero de software con 10 aÃ±os en Python",
    "Desarrollador backend en Java y Spring",
    "CientÃ­fico de datos especializado en ML",
    "Arquitecto de soluciones cloud",
]

real_positions = [
    "Buscamos developer senior python",
    "Necesitamos ingeniero backend java",
    "Requerimos data scientist ml engineer",
]

print("Matching Job <-> Candidate:\n")
for position in real_positions:
    print(f"PosiciÃ³n: {position}")
    results = model.search(position, real_candidates, top_k=2)
    for r in results:
        print(f"  âœ“ {r['candidate']} ({r['similarity']:.1%})")
    print()


# =========================================================================
# OPCIÃ“N 6: Guardar y visualizar resultados
# =========================================================================

"""
Guardar evaluaciÃ³n completa:
"""

evaluator = ModelEvaluator("./model")
results = evaluator.run_full_evaluation()
evaluator.save_results("my_evaluation.json")

# Los resultados estÃ¡n en my_evaluation.json

import json

with open("my_evaluation.json") as f:
    results = json.load(f)

# Ver las mÃ©tricas principales
print(json.dumps(results, indent=2))


# =========================================================================
# BENCHMARK: COMPARATIVA DE RESULTADOS
# =========================================================================

"""
Tabla de referencia de buenos resultados:

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ MÃ©trica                      â•‘ Excelente (âœ…) â•‘ Bueno (ðŸŸ¡)    â•‘ Revisar (âŒ)â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¬â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¬â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¬â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ Velocidad (CPU textos/seg)  â•‘ > 200         â•‘ 50-200       â•‘ < 50      â•‘
â•‘ Velocidad (GPU textos/seg)  â•‘ > 1000        â•‘ 500-1000     â•‘ < 500     â•‘
â•‘ Similitud accuracy          â•‘ > 80%         â•‘ 60-80%       â•‘ < 60%     â•‘
â•‘ MRR (Mean Reciprocal Rank)  â•‘ > 0.8         â•‘ 0.5-0.8      â•‘ < 0.5     â•‘
â•‘ NDCG (Normalized DCG)       â•‘ > 0.75        â•‘ 0.5-0.75     â•‘ < 0.5     â•‘
â•‘ Precision@5                 â•‘ > 0.8         â•‘ 0.6-0.8      â•‘ < 0.6     â•‘
â•‘ Clustering cohesion         â•‘ > 0.7         â•‘ 0.5-0.7      â•‘ < 0.5     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•©â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•©â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•©â•â•â•â•â•â•â•â•â•â•â•â•
"""


# =========================================================================
# INTERPRETACIÃ“N DE RESULTADOS
# =========================================================================

"""
SegÃºn tus mÃ©tricas de entrenamiento en MODEL_INFO.json:

âœ… Tu modelo mostrÃ³:
  â€¢ +33.05% mejora vs modelo original en similarity pairs
  â€¢ Similitud promedio: 0.8795 (vs 0.5490 original)
  â€¢ Exactitud en triplets: 80%

Esto significa que:
  1. Es EXCELENTE para bÃºsqueda y ranking
  2. Los embeddings son discriminativos
  3. Distingue bien entre candidatos similares y diferentes

PrÃ³ximos pasos:
  1. Ejecutar evaluate_model.py para verificar en tu mÃ¡quina
  2. Si hay discrepancias, revisar los datos de test
  3. Considerar fine-tuning adicional si es necesario
"""


# =========================================================================
# MÃS INFORMACIÃ“N
# =========================================================================

"""
Para mÃ¡s detalles, ver:
  â€¢ HOW_TO_MEASURE_PERFORMANCE.py (esta guÃ­a completa)
  â€¢ evaluate_model.py (script de evaluaciÃ³n)
  â€¢ README.md (documentaciÃ³n principal)
  â€¢ CHECKLIST.md (verificaciones)
"""
