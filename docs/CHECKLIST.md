# ‚úÖ CHECKLIST DE CONFIGURACI√ìN

Este documento te ayuda a verificar que todo est√° configurado correctamente.

---

## üîß Requisitos del Sistema

- [ ] Python 3.9+
- [ ] 4GB RAM m√≠nimo (8GB recomendado)
- [ ] 2GB espacio libre (para modelo + deps)
- [ ] pip actualizado
- [ ] Conexi√≥n a internet (solo para primera instalaci√≥n)

**Verificar Python:**
```bash
python --version  # Debe ser 3.9+
pip --version
```

---

## üì¶ Instalaci√≥n

- [ ] Directorio `modelo_entrenado_multiloss_portable/` existe
- [ ] Archivo `requirements.txt` presente
- [ ] Archivo `loader.py` presente
- [ ] Directorio `model/` con pesos (1.08GB)
- [ ] Archivo `api_wrapper.py` presente

**Verificar archivos:**
```bash
ls -lah  # Ver todos los archivos
du -sh model/  # Debe ser ~1.08GB
```

---

## üêç Ambiente Python

- [ ] Virtual environment creado (recomendado)
- [ ] Dependencias instaladas: `pip install -r requirements.txt`
- [ ] Sin errores en la instalaci√≥n

**Verificar instalaci√≥n:**
```bash
pip list | grep "torch\|sentence-transformers\|fastapi"
```

---

## üß™ Prueba B√°sica del Modelo

- [ ] Ejecutado: `python loader.py ./model test`
- [ ] Test 1: Embedding OK
- [ ] Test 2: Similarity OK  
- [ ] Test 3: Search OK
- [ ] Test 4: Get Info OK

**Resultado esperado:**
```
‚úÖ Test 1: Embedding - PASS
‚úÖ Test 2: Similarity - PASS
‚úÖ Test 3: Search - PASS
‚úÖ Test 4: Get Info - PASS
```

---

## üöÄ Opci√≥n 1: Uso Directo en Python

- [ ] Ejecutado: `python examples/example_python.py`
- [ ] Todos los ejemplos pasaron
- [ ] Puedo importar: `from loader import load_model`
- [ ] Puedo crear modelo: `model = load_model("./model")`
- [ ] Puedo generar embedding: `embedding = model.encode("test")`

**Test r√°pido:**
```python
from loader import load_model
model = load_model("./model")
result = model.similarity("python", "java")
print(f"Similitud: {result}")  # Debe imprimir un n√∫mero entre 0-1
```

---

## üåê Opci√≥n 2: API REST

### Servidor API

- [ ] Instaladas dependencias API: `fastapi`, `uvicorn`
- [ ] Ejecutado: `python api_wrapper.py --port 8000`
- [ ] Servidor inici√≥ sin errores
- [ ] Puerto 8000 est√° disponible

**Verificar puerto:**
```bash
netstat -an | grep 8000  # No debe mostrar nada si est√° libre
```

### Health Check

- [ ] `curl http://localhost:8000/health` retorna `{"status":"ok"}`
- [ ] `curl http://localhost:8000/info` retorna metadata del modelo

### Endpoints

- [ ] POST `/embed` - funciona
- [ ] POST `/similarity` - funciona
- [ ] POST `/search` - funciona
- [ ] POST `/cluster` - funciona

**Test r√°pido:**
```bash
curl -X POST http://localhost:8000/embed \
  -H "Content-Type: application/json" \
  -d '{"texts": ["test"]}'
```

### Cliente API

- [ ] Instalado: `pip install requests`
- [ ] Ejecutado: `python examples/example_api_client.py` (en otra terminal)
- [ ] Todos los tests pasaron

---

## üîå Opci√≥n 3: Framework Web (Flask)

- [ ] Instalado: `pip install flask flask-cors`
- [ ] Ejecutado: `python examples/example_flask.py`
- [ ] Servidor Flask inici√≥ en `http://localhost:5000`
- [ ] Endpoints disponibles:
  - [ ] GET `/` - retorna info
  - [ ] POST `/api/search` - busca candidatos
  - [ ] POST `/api/similarity` - calcula similitud
  - [ ] GET `/api/cluster` - agrupa textos

**Test r√°pido:**
```bash
curl -X POST http://localhost:5000/api/search \
  -H "Content-Type: application/json" \
  -d '{"query": "python", "top_k": 3}'
```

---

## üê≥ Opci√≥n 4: Docker

- [ ] Instalado Docker
- [ ] Dockerfile presente
- [ ] Build exitoso: `docker build -t modelo-api:latest .`
- [ ] Container inicia: `docker run -p 8000:8000 modelo-api:latest`
- [ ] API accesible en `http://localhost:8000`

**Verificar Docker:**
```bash
docker --version
docker ps  # Ver containers activos
```

---

## üìö Documentaci√≥n

- [ ] README.md presente y legible
- [ ] INTEGRACION.md presente
- [ ] DEPLOYMENT.md presente
- [ ] MODEL_INFO.json presente
- [ ] examples/ directorio con ejemplos

**Verificar contenido:**
```bash
ls -lah *.md *.json
ls examples/
```

---

## ‚ö° Rendimiento

- [ ] Embedding de 1 texto: < 100ms
- [ ] Embedding de 10 textos: < 500ms
- [ ] Similitud: < 100ms
- [ ] B√∫squeda (100 items): < 10s
- [ ] Memoria: < 3GB en uso

**Benchmark:**
```bash
python -c "
from loader import load_model
import time
model = load_model('./model')
start = time.time()
for _ in range(10):
    model.encode('test')
print(f'Tiempo: {(time.time()-start)*1000/10:.2f}ms por embedding')
"
```

---

## üîê Seguridad (Producci√≥n)

- [ ] API key configurada (si aplica)
- [ ] SSL/TLS habilitado
- [ ] CORS configurado correctamente
- [ ] Rate limiting habilitado
- [ ] Logging configurado
- [ ] Backups configurados

**Verificar HTTPS:**
```bash
curl -I https://tu-servidor.com/health
```

---

## üìä Monitoreo (Producci√≥n)

- [ ] Health checks configurados
- [ ] Logging iniciado
- [ ] M√©tricas recolectadas (si aplica)
- [ ] Alertas configuradas
- [ ] Dashboard disponible (si aplica)

**Ver logs:**
```bash
journalctl -u modelo-api -f  # Si usas systemd
docker logs modelo_api  # Si usas docker
```

---

## üö® Troubleshooting: Errores Comunes

### Error: ModuleNotFoundError

```
‚ùå ModuleNotFoundError: No module named 'torch'
```

**Soluci√≥n:**
```bash
pip install torch sentence-transformers
```

### Error: Model not found

```
‚ùå FileNotFoundError: ./model/pytorch_model.bin
```

**Soluci√≥n:**
```bash
ls -la model/  # Verificar que existan los archivos
du -sh model/  # Debe ser ~1.08GB
```

### Error: Port already in use

```
‚ùå OSError: [Errno 48] Address already in use
```

**Soluci√≥n:**
```bash
# Opci√≥n 1: Liberar puerto
lsof -i :8000
kill -9 <PID>

# Opci√≥n 2: Usar puerto diferente
python api_wrapper.py --port 9000
```

### Error: Out of memory

```
‚ùå RuntimeError: CUDA out of memory
```

**Soluci√≥n:**
```bash
# Forzar CPU
CUDA_VISIBLE_DEVICES=-1 python api_wrapper.py

# O en Python
import os
os.environ['CUDA_VISIBLE_DEVICES'] = '-1'
```

### Error: Slow performance

```
‚ö†Ô∏è Cada embedding tarda > 1 segundo
```

**Soluci√≥n:**
```bash
# Verificar si usa GPU
python -c "import torch; print(torch.cuda.is_available())"

# Usar GPU si disponible
python api_wrapper.py --device cuda

# O hacer batch processing
model.encode(["text1", "text2", "text3"])  # M√°s r√°pido
```

---

## üìã Antes de Ir a Producci√≥n

- [ ] ‚úÖ Pas√≥ todo el checklist anterior
- [ ] ‚úÖ Testeado en desarrollo 2+ veces
- [ ] ‚úÖ Documentaci√≥n revisada
- [ ] ‚úÖ Logs configurados
- [ ] ‚úÖ Backups en lugar
- [ ] ‚úÖ Rollback plan listo
- [ ] ‚úÖ Equipo capacitado
- [ ] ‚úÖ Monitoreo en lugar
- [ ] ‚úÖ Alertas configuradas

---

## üìù Notas Finales

### Despu√©s de Instalaci√≥n

- [ ] Guardar esta copia del modelo en lugar seguro
- [ ] Anotar fecha de instalaci√≥n
- [ ] Documentar cualquier customizaci√≥n
- [ ] Setup backups autom√°ticos

### Mantenimiento Regular

- [ ] Revisar logs mensualmente
- [ ] Verificar uso de memoria
- [ ] Actualizar dependencias (cuando sea seguro)
- [ ] Hacer backups semanales

### En Caso de Problemas

1. Revisa logs: `journalctl -u modelo-api -f`
2. Reinstala dependencias: `pip install -r requirements.txt`
3. Verifica espacio: `df -h`
4. Reinicia servicio: `systemctl restart modelo-api`
5. Consulta documentaci√≥n

---

## üéØ Resumen: Estado del Deployment

| Componente | Estado | Verificado |
|-----------|--------|-----------|
| Python | ‚úÖ | [ ] |
| Dependencias | ‚úÖ | [ ] |
| Modelo | ‚úÖ | [ ] |
| Loader | ‚úÖ | [ ] |
| API | ‚úÖ | [ ] |
| Ejemplos | ‚úÖ | [ ] |
| Docker | ‚úÖ | [ ] |
| Documentaci√≥n | ‚úÖ | [ ] |
| Seguridad | üîÑ | [ ] |
| Monitoreo | üîÑ | [ ] |

---

## üìû Soporte

Si algo falla:

1. **Logs**: Revisa los logs del servicio
2. **Documentaci√≥n**: Consulta README.md o INTEGRACION.md
3. **Ejemplos**: Ejecuta los ejemplos en `examples/`
4. **Health Check**: Verifica `curl http://localhost:8000/health`

---

*√öltima actualizaci√≥n: 8 de Enero, 2026*

**Estado actual:** ‚úÖ **LISTO PARA PRODUCCI√ìN** (despu√©s de completar checklist)
